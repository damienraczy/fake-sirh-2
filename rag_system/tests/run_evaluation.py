# tests/run_evaluation.py
import json
import os
import sys
import time
from pathlib import Path

# Ajouter le r√©pertoire parent au path pour que les imports fonctionnent
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.insert(0, parent_dir)
grand_parent_dir = os.path.dirname(parent_dir)
sys.path.insert(0, grand_parent_dir)

from core.config import load_config, get_config
from rag.chain import SIRHRAGChain
from rag.config import RAGConfig

# --- Fonctions de validation ---

def check_route(test_case, result):
    """V√©rifie si la route choisie est la bonne."""
    expected = test_case['expected_route']
    actual = result['route']
    # G√©rer les cas de fallback (ex: "SQL_FALLBACK_VECTOR" commence par "SQL")
    return actual.startswith(expected)

def check_keywords(test_case: str, result: list):
    """
    V√©rifie si la r√©ponse contient les √©l√©ments attendus.
    Pour les tests VECTOR, on est plus flexible : on v√©rifie si au moins
    les mots-cl√©s principaux (noms propres) sont pr√©sents.
    """
    answer = result['answer'].lower()
    keywords = [str(k).lower() for k in test_case['expected_keywords']]

    # Pour les tests non-VECTOR, tous les mots-cl√©s doivent √™tre pr√©sents
    if test_case['expected_route'] != 'VECTOR':
        return all(keyword in answer for keyword in keywords)
    
    # Pour les tests VECTOR, la r√©ponse peut √™tre formul√©e de diverses mani√®res.
    # On v√©rifie seulement la pr√©sence des noms propres pour s'assurer que le bon document a √©t√© trouv√©.
    # Ici, on consid√®re que les deux premiers mots-cl√©s sont le pr√©nom et le nom.
    if len(keywords) >= 2:
        return keywords[0] in answer and keywords[1] in answer
    
    return False

# --- Script principal ---

def run_tests():
    """Charge le dataset, initialise le RAG et lance les tests."""
    print("üöÄ D√©marrage de l'√©valuation automatis√©e du RAG...")

    # 1. Charger la configuration
    config_path = os.path.join(grand_parent_dir, 'config.yaml')
    load_config(config_path)
    base_config = get_config()
    rag_config = RAGConfig.from_base_config(base_config)

    # 2. Charger le Golden Dataset
    dataset_path = os.path.join(current_dir, 'golden_dataset.json')
    try:
        with open(dataset_path, 'r', encoding='utf-8') as f:
            dataset = json.load(f)
        print(f"‚úÖ Golden Dataset charg√© avec {len(dataset)} cas de test.")
    except FileNotFoundError:
        print(f"‚ùå Erreur : Fichier 'golden_dataset.json' introuvable.")
        print("   Veuillez d'abord le g√©n√©rer avec 'python tests/generate_golden_dataset.py'")
        return

    # 3. Initialiser la cha√Æne RAG
    print("\nINITIALISATION DE LA CHAINE RAG...")
    rag_chain = SIRHRAGChain(rag_config)
    print("...CHAINE RAG INITIALIS√âE.\n")

    # 4. Ex√©cuter les tests
    results = []
    total_tests = len(dataset)
    passed_tests = 0

    print("--- LANCEMENT DES TESTS ---")
    for i, test_case in enumerate(dataset):
        print(f"\nüß™ Test {i + 1}/{total_tests} : ID = {test_case['question_id']}")
        print(f"   Question : \"{test_case['question']}\"")
        
        start_time = time.time()
        rag_result = rag_chain.query(test_case['question'])
        latency = time.time() - start_time

        # √âvaluation
        route_ok = check_route(test_case, rag_result)
        keywords_ok = check_keywords(test_case, rag_result)
        test_passed = route_ok and keywords_ok

        if test_passed:
            passed_tests += 1
            print(f"   ‚û°Ô∏è R√©sultat : ‚úÖ PASS")
        else:
            print(f"   ‚û°Ô∏è R√©sultat : ‚ùå FAIL")

        # Affichage des d√©tails en cas d'√©chec
        if not route_ok:
            print(f"     - Erreur de routage : Attendu='{test_case['expected_route']}', Obtenu='{rag_result['route']}'")
        if not keywords_ok:
            print(f"     - Mots-cl√©s manquants. Attendu : {test_case['expected_keywords']}")
            print(f"     - R√©ponse obtenue : \"{rag_result['answer'][:150]}...\"")

        results.append({
            "id": test_case['question_id'],
            "passed": test_passed,
            "latency": f"{latency:.2f}s"
        })
    
    # 5. Afficher le rapport final
    print("\n\n--- RAPPORT D'√âVALUATION ---")
    print(f"Total des tests : {total_tests}")
    print(f"Tests r√©ussis   : {passed_tests} ({(passed_tests/total_tests)*100:.2f}%)")
    print(f"Tests √©chou√©s   : {total_tests - passed_tests}")
    print("----------------------------")
    for res in results:
        status = "‚úÖ PASS" if res['passed'] else "‚ùå FAIL"
        print(f"- {res['id']:<15} | Statut: {status:<7} | Latence: {res['latency']}")
    print("----------------------------")
    
    if total_tests != passed_tests:
        # Permet de signaler un √©chec √† une CI/CD
        sys.exit(1)

if __name__ == "__main__":
    run_tests()